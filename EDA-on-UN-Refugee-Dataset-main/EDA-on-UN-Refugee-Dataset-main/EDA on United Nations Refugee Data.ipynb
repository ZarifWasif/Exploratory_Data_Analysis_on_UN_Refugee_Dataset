{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Country of origin</th>\n",
       "      <th>Country of origin (ISO)</th>\n",
       "      <th>Country of asylum</th>\n",
       "      <th>Country of asylum (ISO)</th>\n",
       "      <th>Refugees under UNHCR's mandate</th>\n",
       "      <th>Asylum-seekers</th>\n",
       "      <th>IDPs of concern to UNHCR</th>\n",
       "      <th>Other people in need of international protection</th>\n",
       "      <th>Stateless persons</th>\n",
       "      <th>Host Community</th>\n",
       "      <th>Others of concern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>180000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>282000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>BEL</td>\n",
       "      <td>55000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>168511</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>DNK</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Country of origin Country of origin (ISO) Country of asylum  \\\n",
       "0  1951          Unknown                      UNK         Australia   \n",
       "1  1951          Unknown                      UNK           Austria   \n",
       "2  1951          Unknown                      UNK           Belgium   \n",
       "3  1951          Unknown                      UNK            Canada   \n",
       "4  1951          Unknown                      UNK           Denmark   \n",
       "\n",
       "  Country of asylum (ISO)  Refugees under UNHCR's mandate  Asylum-seekers  \\\n",
       "0                     AUS                          180000               0   \n",
       "1                     AUT                          282000               0   \n",
       "2                     BEL                           55000               0   \n",
       "3                     CAN                          168511               0   \n",
       "4                     DNK                            2000               0   \n",
       "\n",
       "   IDPs of concern to UNHCR Other people in need of international protection  \\\n",
       "0                         0                                                -   \n",
       "1                         0                                                -   \n",
       "2                         0                                                -   \n",
       "3                         0                                                -   \n",
       "4                         0                                                -   \n",
       "\n",
       "   Stateless persons Host Community  Others of concern  \n",
       "0                  0              -                  0  \n",
       "1                  0              -                  0  \n",
       "2                  0              -                  0  \n",
       "3                  0              -                  0  \n",
       "4                  0              -                  0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset (replace 'your_dataset.csv' with the actual file path or URL)\n",
    "dataset_path = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processed and saved to G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data cleaned.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Country of origin</th>\n",
       "      <th>Country of asylum</th>\n",
       "      <th>Asylum-seekers</th>\n",
       "      <th>IDPs of concern to UNHCR</th>\n",
       "      <th>Other people in need of international protection</th>\n",
       "      <th>Stateless persons</th>\n",
       "      <th>Host Community</th>\n",
       "      <th>Others of concern</th>\n",
       "      <th>Refugees under UNHCR's mandate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Austria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Country of origin Country of asylum  Asylum-seekers  \\\n",
       "0  1951          Unknown          Australia               0   \n",
       "1  1951          Unknown            Austria               0   \n",
       "2  1951          Unknown            Belgium               0   \n",
       "3  1951          Unknown             Canada               0   \n",
       "4  1951          Unknown            Denmark               0   \n",
       "\n",
       "   IDPs of concern to UNHCR  Other people in need of international protection  \\\n",
       "0                         0                                                 0   \n",
       "1                         0                                                 0   \n",
       "2                         0                                                 0   \n",
       "3                         0                                                 0   \n",
       "4                         0                                                 0   \n",
       "\n",
       "   Stateless persons  Host Community  Others of concern  \\\n",
       "0                  0               0                  0   \n",
       "1                  0               0                  0   \n",
       "2                  0               0                  0   \n",
       "3                  0               0                  0   \n",
       "4                  0               0                  0   \n",
       "\n",
       "   Refugees under UNHCR's mandate  \n",
       "0                          180000  \n",
       "1                          282000  \n",
       "2                           55000  \n",
       "3                          168511  \n",
       "4                            2000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to replace non-numeric values with zero\n",
    "def replace_non_numeric(data):\n",
    "    if pd.api.types.is_numeric_dtype(data):\n",
    "        return data.fillna(0)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "def process_csv(input_file, output_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Drop the \"Country of origin (ISO)\" and \"Country of asylum (ISO)\" columns\n",
    "    df.drop(columns=['Country of origin (ISO)', 'Country of asylum (ISO)'], inplace=True)\n",
    "\n",
    "    # Replace non-numeric values with zero in specified columns\n",
    "    cols_to_replace_zero = ['Refugees under UNHCR\\'s mandate', 'Asylum-seekers', 'IDPs of concern to UNHCR', \n",
    "                            'Other people in need of international protection', 'Stateless persons', \n",
    "                            'Host Community', 'Others of concern']\n",
    "    for col in cols_to_replace_zero:\n",
    "        df[col] = replace_non_numeric(df[col])\n",
    "\n",
    "    # Move \"Refugees under UNHCR's mandate\" column to last position\n",
    "    df = df[[col for col in df if col != \"Refugees under UNHCR's mandate\"] + [\"Refugees under UNHCR's mandate\"]]\n",
    "\n",
    "    # Save the updated dataset to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(\"Dataset processed and saved to\", output_file)\n",
    "\n",
    "# Example usage\n",
    "input_file = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data.csv\"  # Change this to the path of your input CSV file\n",
    "output_file = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data cleaned.csv\"  # Change this to the desired output CSV file path\n",
    "process_csv(input_file, output_file)\n",
    "\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with one-hot encoding and binary encoding saved to G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data cleaned_Augmented.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "input_file = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data cleaned.csv\"  # Change this to the path of your input CSV file\n",
    "output_file = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data cleaned_Augmented.csv\"  # Change this to the desired output CSV file path\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Perform one-hot encoding for \"Country of origin\" and \"Country of asylum\" columns\n",
    "df = pd.get_dummies(df, columns=['Country of origin', 'Country of asylum'], drop_first=True, dummy_na=False)\n",
    "\n",
    "# Move \"Refugees under UNHCR's mandate\" column to the rightmost position\n",
    "column_order = [col for col in df.columns if col != \"Refugees under UNHCR's mandate\"] + [\"Refugees under UNHCR's mandate\"]\n",
    "df = df[column_order]\n",
    "\n",
    "# Replace True/False values with 1/0\n",
    "df = df.replace({True: 1, False: 0})\n",
    "\n",
    "# Save the updated dataset to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"Dataset with one-hot encoding and binary encoding saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 125689\n",
      "Number of columns: 413\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "input_file = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data cleaned_Augmented.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Get the number of rows and columns\n",
    "num_rows, num_columns = df.shape\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped due to high correlation: set()\n",
      "Number of columns dropped: 0\n",
      "Filtered dataset saved to G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the encoded dataset (replace 'your_dataset.csv' with the actual file path or URL)\n",
    "input_file = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data cleaned_Augmented.csv\"\n",
    "df_encoded = pd.read_csv(input_file)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_encoded.corr()\n",
    "\n",
    "# Set a correlation threshold (adjust as needed)\n",
    "correlation_threshold = 0.8\n",
    "\n",
    "# Identify highly correlated variables\n",
    "highly_correlated_vars = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_vars.add(colname)\n",
    "\n",
    "# Print the columns to be dropped\n",
    "print(f\"Columns to be dropped due to high correlation: {highly_correlated_vars}\")\n",
    "\n",
    "# Count the number of columns to be dropped\n",
    "num_columns_dropped = len(highly_correlated_vars)\n",
    "print(f\"Number of columns dropped: {num_columns_dropped}\")\n",
    "\n",
    "# Drop highly correlated variables from the dataset\n",
    "df_filtered = df_encoded.drop(columns=highly_correlated_vars)\n",
    "\n",
    "# Save the filtered dataset to a new CSV file\n",
    "output_file = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "print(\"Filtered dataset saved to\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4301288995.995337\n",
      "Root Mean Squared Error (RMSE): 65584.21300888908\n",
      "R-squared (R2): 0.14162693395981607\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer  # Import the imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = linear_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4301297376.0313425\n",
      "Root Mean Squared Error (RMSE): 65584.27689645851\n",
      "R-squared (R2): 0.141625261624568\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a Ridge Regression model\n",
    "ridge_reg_model = Ridge(alpha=1.0)  # You can adjust the regularization strength (alpha) if needed\n",
    "ridge_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = ridge_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4301447634.230946\n",
      "Root Mean Squared Error (RMSE): 65585.42242168564\n",
      "R-squared (R2): 0.14159527582458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rimit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.792e+10, tolerance: 4.428e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a Lasso Regression model\n",
    "lasso_reg_model = Lasso(alpha=1.0)  # You can adjust the regularization strength (alpha) if needed\n",
    "lasso_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = lasso_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4465953410.611945\n",
      "Root Mean Squared Error (RMSE): 66827.78920936967\n",
      "R-squared (R2): 0.10876620347326504\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train an ElasticNet Regression model\n",
    "elastic_net_model = ElasticNet(alpha=1.0, l1_ratio=0.5)  # You can adjust alpha and l1_ratio as needed\n",
    "elastic_net_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = elastic_net_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3715896262.471197\n",
      "Root Mean Squared Error (RMSE): 60958.15173109497\n",
      "R-squared (R2): 0.2584489740460769\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a Decision Tree Regression model\n",
    "decision_tree_model = DecisionTreeRegressor(max_depth=5)  # You can adjust the max_depth parameter as needed\n",
    "decision_tree_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = decision_tree_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3060083468.808533\n",
      "Root Mean Squared Error (RMSE): 55318.02119389786\n",
      "R-squared (R2): 0.38932416959064753\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a Random Forest Regression model\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, max_depth=5)  # You can adjust parameters as needed\n",
    "random_forest_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = random_forest_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2067040105.3634045\n",
      "Root Mean Squared Error (RMSE): 45464.7127491575\n",
      "R-squared (R2): 0.5874977118438813\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a Gradient Boosting Regression model\n",
    "gradient_boosting_model = GradientBoostingRegressor(n_estimators=100, max_depth=5)  # You can adjust parameters as needed\n",
    "gradient_boosting_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = gradient_boosting_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a Support Vector Regression model\n",
    "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)  # You can adjust parameters as needed\n",
    "svr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = svr_model.predict(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Train a K-Nearest Neighbors Regression model\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)  # You can adjust the number of neighbors (n_neighbors) as needed\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural network regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Load the dataset without outliers and with one-hot encoding\n",
    "input_file_path_encoded = r\"G:\\My Drive\\Internship project\\United Nations Refugee Data.csv\\United Nations Refugee Data filtered.csv\"\n",
    "df_encoded = pd.read_csv(input_file_path_encoded)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Refugees under UNHCR\\'s mandate'])\n",
    "y = df_encoded['Refugees under UNHCR\\'s mandate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Build the Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
